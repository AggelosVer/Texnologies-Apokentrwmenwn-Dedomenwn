# Αναφορά Εργασίας: Υλοποίηση Chord και Pastry DHT

## 1. Εισαγωγή και Στόχος

Σε αυτή την εργασία, ο σκοπός μας ήταν να φτιάξουμε και να συγκρίνουμε δύο συστήματα P2P (Peer-to-Peer), το **Chord** και το **Pastry**. Αυτά τα συστήματα ανήκουν στην κατηγορία των **Distributed Hash Tables (DHT)** και μας επιτρέπουν να αποθηκεύουμε πληροφορίες σε ένα δίκτυο κόμβων χωρίς να υπάρχει ένας κεντρικός server που να τα ελέγχει όλα.

### Τι θέλουμε να πετύχουμε;
Η βασική ιδέα είναι ότι έχουμε ένα μεγάλο σύνολο δεδομένων (στην περίπτωσή μας ταινίες από το TMDB) και θέλουμε να τις μοιράσουμε σε διάφορους υπολογιστές (κόμβους). Ο κάθε κόμβος πρέπει να ξέρει πού να βρει την πληροφορία που ψάχνουμε, αλλά χωρίς να χρειάζεται να ξέρει τα πάντα για όλο το δίκτυο.

Οι κύριοι στόχοι μας ήταν:
- Να φτιάξουμε τον αλγόριθμο του **Chord** που χρησιμοποιεί έναν δακτύλιο για να οργανώνει τους κόμβους.
- Να φτιάξουμε τον αλγόριθμο του **Pastry** που χρησιμοποιεί προθέματα (prefixes) για να βρίσκει πιο γρήγορα τους κόμβους.
- Να μετρήσουμε πόσα "βήματα" (hops) χρειάζεται το κάθε σύστημα για να βρει μια ταινία.
- Να δούμε πώς συμπεριφέρεται το δίκτυο όταν προσθέτουμε ή αφαιρούμε κόμβους (Join/Leave).

### Πώς το υλοποιήσαμε;
Χρησιμοποιήσαμε **Python** και προσομοιώσαμε το δίκτυο με:
- **Threads** για τους κόμβους, ώστε να τρέχουν ταυτόχρονα.
- **Sockets** για να επικοινωνούν μεταξύ τους με μηνύματα.
- **B+ Trees** τοπικά σε κάθε κόμβο, για να μπορούμε να κάνουμε γρήγορη αναζήτηση μέσα στα δεδομένα που κατέχει ο κάθε peer.

## 2. Κατακερματισμός και Γενική Λειτουργία DHT

Πριν δούμε την υλοποίηση, πρέπει να καταλάβουμε πώς οργανώνονται τα δεδομένα μας στο δίκτυο.

### 2.1 Αναγνωριστικά και Hashing (SHA-1)
Το πρώτο βήμα είναι να δώσουμε μια "ταυτότητα" (ID) τόσο στους κόμβους (τον κάθε υπολογιστή) όσο και στα κλειδιά μας (τους τίτλους των ταινιών). Για να το κάνουμε αυτό ομοιόμορφα, χρησιμοποιήσαμε τη συνάρτηση κατακερματισμού **SHA-1**.

Στον κώδικα μας (αρχείο `dht_hash.py`), η συνάρτηση `hash_key` παίρνει ένα string (π.χ. τον τίτλο "Inception") και βγάζει έναν αριθμό 160-bit.

```python
# Απόσπασμα από dht_hash.py
def hash_key(self, key: str) -> int:
    # Κανονικοποίηση (πεζά γράμματα) και κωδικοποίηση
    normalized_key = key.strip().lower()
    key_bytes = normalized_key.encode('utf-8')
    # Παραγωγή SHA-1 hash
    hash_obj = hashlib.sha1(key_bytes)
    # Μετατροπή σε ακέραιο στο εύρος του δακτυλίου (2^160)
    hash_int = int(hash_obj.hexdigest(), 16)
    return hash_int % self.ring_size
```

Αυτή η μέθοδος είναι πολύ σημαντική γιατί διασφαλίζει ότι οι ταινίες θα "μοιραστούν" ομοιόμορφα σε όλο το δίκτυο.

### 2.2 Παράδειγμα Hashing Ταινιών
Τρέχοντας το demo του hasher (`python dht_hash.py`), βλέπουμε πώς μετατρέπονται οι τίτλοι σε IDs:

```text
Title: The Shawshank Redemption
  Decimal ID: 11775657401622277183644157439191363171212126089
  Hex ID:     021009eae1cef4159785df5e4fedd0de03cce789
  Pastry Prefix (8 digits): 021009ea

Title: Inception
  Decimal ID: 684467261159164595678055514253760854109088231948
  Hex ID:     77e492d109db59d1fe3093815f1ee944242db20c
  Pastry Prefix (8 digits): 77e492d1
```

## 3. Chord: Θεωρία και Υλοποίηση

### 3.1 Η δομή του Δακτυλίου (Theory)
Ο **Chord** βλέπει το δίκτυο σαν έναν κύκλο (δακτύλιο) από IDs από $0$ έως $2^{160}-1$. Κάθε κόμβος μπαίνει σε μια θέση στον κύκλο και είναι υπεύθυνος για όλα τα κλειδιά που βρίσκονται μεταξύ αυτού και του προηγούμενου κόμβου του.

### 3.2 Ο Finger Table (Code)
Κάθε κόμβος `ChordNode` διατηρεί μια λίστα από `m_bits` (160) άλλους κόμβους. Αυτοί οι κόμβοι είναι οι "σταθμοί" που μας επιτρέπουν να πηδάμε στο μισό της απόστασης που απομένει μέχρι το στόχο.

Στον κώδικα (αρχείο `chord_node.py`):
```python
self.finger_table: List['ChordNode'] = [self] * self.m_bits
```

### 3.3 Αλγόριθμος Αναζήτησης (find_successor)
Όταν ψάχνουμε μια ταινία, ρωτάμε τον κόμβο: "Ποιος είναι ο επόμενος για αυτό το ID;". 

```python
def find_successor(self, id: int) -> 'ChordNode':
    # Αν το ID είναι ανάμεσα σε μένα και τον επόμενό μου, ο επόμενος είναι ο στόχος
    if self.hasher.in_range(id, self.id, self.successor.id, inclusive_start=False, inclusive_end=True):
        return self.successor
    else:
        # Αλλιώς, βρες στον Finger Table τον πιο κοντινό προηγούμενο
        n0 = self.closest_preceding_node(id)
        if n0 is self:
            return self.successor
        return n0.find_successor(id)
```

### 3.4 Συντήρηση και Join
Επειδή οι κόμβοι μπορεί να μπουν ή να βγουν από το δίκτυο, έχουμε τον μηχανισμό **Stabilization** που "φτιάχνει" τον δακτύλιο συνεχώς ρωτώντας τον successor "ποιος είναι ο predecessor σου;".

## 4. Pastry: Θεωρία και Υλοποίηση

### 4.1 Δρομολόγηση βάσει Προθέματος (Prefix Routing)
Ο **Pastry** δουλεύει διαφορετικά. Το ID του κάθε κόμβου βλέπεται σαν μια σειρά από ψηφία στο 16-αδικό σύστημα (hexadecimal). Η δρομολόγηση γίνεται προσπαθώντας σε κάθε βήμα να βρούμε έναν κόμβο που να έχει μεγαλύτερο κοινό πρόθεμα (prefix) με το κλειδί που ψάχνουμε.

### 4.2 Routing Table και Leaf Set (Code)
Ο Pastry χρησιμοποιεί δύο βασικές δομές:
- **Leaf Set**: Οι πιο κοντινοί κόμβοι αριθμητικά (μικρότεροι και μεγαλύτεροι).
- **Routing Table**: Ένας πίνακας με κόμβους που μοιράζονται κοινά προθέματα.

```python
# Από το pastry_node.py
self.leaf_smaller: List['PastryNode'] = []
self.leaf_larger: List['PastryNode'] = []
self.routing_table: Dict[int, Dict[int, 'PastryNode']] = {}
```

### 4.3 Ο αλγόριθμος route
Ο αλγόριθμος ελέγχει πρώτα αν το ID είναι μέσα στο εύρος του **Leaf Set**. Αν όχι, πάει στο **Routing Table**.

```python
def route(self, key_id: int, hops: int = 0, visited: Optional[set] = None) -> tuple['PastryNode', int]:
    # ... έλεγχοι ασφαλείας ...
    key_hex = self.hasher.get_hex_id(key_id, digits=self.m_bits//4)
    shared_prefix = self._shared_prefix_length(key_hex)
    
    # 1. Έλεγχος στο Leaf Set
    if self.is_in_leaf_set_range(key_id):
        closest = self.find_closest_in_leaf_set(key_id)
        if closest is self: return (self, hops)
        return closest.route(key_id, hops + 1, visited)

    # 2. Χρήση του Routing Table (Prefix Matching)
    if shared_prefix < self.num_rows:
        next_digit = int(key_hex[shared_prefix], 16)
        if shared_prefix in self.routing_table and next_digit in self.routing_table[shared_prefix]:
            next_node = self.routing_table[shared_prefix][next_digit]
            return next_node.route(key_id, hops + 1, visited)
    
    # 3. Αν τίποτα δεν δουλέψει, πάμε στον πιο κοντινό του Leaf Set
    closest_leaf = self.find_closest_in_leaf_set(key_id)
    return closest_leaf.route(key_id, hops + 1, visited)
```

### 4.4 Είσοδος στο Δίκτυο (Join)
Όταν ένας κόμβος μπαίνει στο δίκτυο του Pastry, "ανακαλύπτει" όλους τους κόμβους που βρίσκονται στη διαδρομή προς το ID του και ενημερώνει τους δικούς του πίνακες αλλά και τους πίνακες των άλλων.

```python
def join(self, introducer: Optional['PastryNode'] = None):
    for node in all_nodes:
        self.add_node(node)
        node.add_node(self)
```
Με αυτόν τον τρόπο, το δίκτυο αυτο-οργανώνεται πολύ γρήγορα.

## 5. Τοπική Ευρετηρίαση με B+ Trees

Αφού το δίκτυο (Chord ή Pastry) βρει ποιος κόμβος είναι υπεύθυνος για μια ταινία, ο κόμβος αυτός πρέπει να ψάξει γρήγορα στα δικά του δεδομένα. Όπως ζητάει η εκφώνηση, κάθε peer είναι εξοπλισμένος με ένα **B+ Tree** για να διαχειρίζεται τοπικά τις ταινίες του.

### 5.1 Γιατί B+ Tree;
Επιλέξαμε το B+ Tree γιατί προσφέρει:
- **Ταχύτητα**: Αναζήτηση, εισαγωγή και διαγραφή σε χρόνο $O(\log n)$.
- **Range Queries**: Είναι ιδανικό για να βρίσκουμε εύκολα ένα εύρος τιμών (π.χ. ταινίες με βαθμολογία από 8 έως 10), κάτι που μια απλή Hash Table δεν μπορεί να κάνει αποδοτικά.

### 5.2 Υλοποίηση στον Κόμβο
Στον κώδικα μας (αρχείο `bplus_tree.py`), το δέντρο αρχικοποιείται με μια συγκεκριμένη τάξη (order=10).

```python
# Αρχικοποίηση στην κλάση ChordNode ή PastryNode
self.data = BPlusTree(order=10)
```

### 5.3 Range Queries και Φιλτράρισμα (Code)
Το πιο σημαντικό κομμάτι είναι η δυνατότητα να κάνουμε σύνθετες αναζητήσεις (filtering) αφού εντοπίσουμε τον σωστό peer. Για παράδειγμα, μπορούμε να ζητήσουμε από έναν κόμβο όλες τις ταινίες που έχει και ανήκουν σε ένα εύρος δημοφιλίας.

```python
# Από το bplus_tree.py
def range_query(self, start_key, end_key) -> List[Any]:
    results = []
    # Βρες το πρώτο φύλλο που περιέχει το start_key
    node = self._find_leaf(start_key)
    
    while node:
        for i, key in enumerate(node.keys):
            if start_key <= key <= end_key:
                results.append(node.values[i])
            elif key > end_key:
                return results
        # Πήγαινε στο επόμενο φύλλο (τα φύλλα είναι συνδεδεμένα μεταξύ τους)
        node = node.next_leaf
    return results
```

Με αυτή τη δομή, ο κάθε κόμβος δεν είναι απλά μια αποθήκη, αλλά ένα μικρό "database engine" που μπορεί να απαντήσει σε ερωτήσεις όπως: *"Δώσε μου τις ταινίες που έχεις με popularity πάνω από 50"*.

## 6. Δικτυακή Επικοινωνία (Sockets & JSON)

Για να λειτουργήσει το DHT ως κατανεμημένο σύστημα, έπρεπε οι κόμβοι να μπορούν να μιλάνε μεταξύ τους. Χρησιμοποιήσαμε **TCP Sockets** και ένα πρωτόκολλο ανταλλαγής μηνυμάτων σε μορφή **JSON**.

### 6.1 Δομή Μηνύματος (Message Protocol)
Κάθε επικοινωνία στο δίκτυο γίνεται μέσω ενός αντικειμένου `Message`. Στον κώδικα μας (αρχείο `message_protocol.py`), ορίσαμε διάφορους τύπους μηνυμάτων όπως `FIND_SUCCESSOR`, `NOTIFY`, `LOOKUP`, κλπ.

Ένα τυπικό μήνυμα περιέχει:
- **`msg_type`**: Τι θέλουμε να κάνουμε.
- **`sender_address`**: Ποιος στέλνει το μήνυμα.
- **`payload`**: Τα δεδομένα του αιτήματος (π.χ. το key_id που ψάχνουμε).

```python
# Από το message_protocol.py
class Message:
    def to_json(self) -> str:
        # Μετατροπή του αντικειμένου σε string για αποστολή
        return json.dumps(self.to_dict())
    
    @classmethod
    def from_bytes(cls, data: bytes) -> 'Message':
        # Μετατροπή των bytes που λάβαμε πίσω σε αντικείμενο Message
        length = int.from_bytes(data[:4], byteorder='big')
        json_str = data[4:4+length].decode('utf-8')
        return cls.from_json(json_str)
```

### 6.2 Ο Network Client/Server
Ο κάθε κόμβος τρέχει έναν Server που "ακούει" σε μια θύρα (port) και έναν Client που στέλνει αιτήματα σε άλλους κόμβους. Όταν ένας κόμβος θέλει να ρωτήσει κάτι, ανοίγει μια σύνδεση, στέλνει το JSON μήνυμα και περιμένει την απάντηση.

Στο αρχείο `network_node_tcp.py`, βλέπουμε πώς γίνεται η διαχείριση των αιτημάτων:

```python
def _handle_connection(self, conn, addr):
    # Λήψη δεδομένων από το socket
    data = conn.recv(self.buffer_size)
    message = Message.from_bytes(data)
    
    # Επεξεργασία του μηνύματος ανάλογα με τον τύπο του
    result = self.process_message(message)
    
    # Αποστολή απάντησης πίσω
    response = create_response(message, result)
    conn.sendall(response.to_bytes())
```

Αυτό το σύστημα μας επιτρέπει να "τρέχουμε" το DHT σε πολλούς διαφορετικούς υπολογιστές, αρκεί να ξέρουν τις IP διευθύνσεις και τα Ports των άλλων κόμβων.

## 7. Παράλληλα Ερωτήματα για $K$ Ταινίες

Ένα από τα ειδικά ζητήματα της εργασίας ήταν η δυνατότητα να βρίσκουμε ταυτόχρονα τη δημοτικότητα (popularity) για **$K$ διαφορετικές ταινίες** (π.χ. $K=10$), με τα ονόματα των ταινιών να δίνονται από τον χρήστη.

### 7.1 Γιατί παράλληλα;
Αν κάναμε τις αναζητήσεις μία-μία (σειριακά), η συνολική καθυστέρηση θα ήταν το άθροισμα των χρόνων κάθε αναζήτησης. Χρησιμοποιώντας **Multi-threading**, μπορούμε να στέλνουμε όλα τα αιτήματα ταυτόχρονα στο δίκτυο και να περιμένουμε τις απαντήσεις παράλληλα.

### 7.2 Υλοποίηση με ThreadPoolExecutor
Για να το πετύχουμε αυτό, φτιάξαμε την κλάση `ParallelLookupExecutor` (αρχείο `parallel_lookup_executor.py`). Αυτή χρησιμοποιεί ένα "pool" από threads για να διαχειριστεί τις αναζητήσεις.

```python
# Από το parallel_lookup_executor.py
def lookup_popularity(self, titles: List[str]):
    results = {}
    with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
        # Στέλνουμε όλα τα ερωτήματα ταυτόχρονα
        future_map = {executor.submit(self._lookup_single, title): title for title in titles}
        
        # Μαζεύουμε τα αποτελέσματα καθώς έρχονται
        for future in as_completed(future_map):
            title, popularity, elapsed, error = future.result()
            results[title] = popularity
    return results
```

### 7.3 Πλεονεκτήματα
Με αυτή την προσέγγιση:
- **Μειώνεται ο χρόνος αναμονής**: Ο συνολικός χρόνος είναι σχεδόν ίσος με τον χρόνο της πιο αργής μεμονωμένης αναζήτησης, αντί για το άθροισμα όλων.
- **Καλύτερη αξιοποίηση του δικτύου**: Το δίκτυο δουλεύει παράλληλα, στέλνοντας μηνύματα σε διαφορετικούς κόμβους την ίδια στιγμή.

Αυτή η λειτουργία είναι πολύ χρήσιμη για εφαρμογές όπως η πρόβλεψη τάσεων ή η δημιουργία λιστών με "trending" ταινίες, όπου χρειαζόμαστε μαζικά δεδομένα γρήγορα.

## 8. Δεδομένα και Αντοχή σε Σφάλματα (Replication)

Για να δοκιμάσουμε το σύστημα σε ρεαλιστικές συνθήκες, χρησιμοποιήσαμε ένα σύνολο δεδομένων με ταινίες και υλοποιήσαμε μηχανισμούς για να μην χάνονται τα δεδομένα αν κάποιος κόμβος "πέσει".

### 8.1 Το Dataset (TMDB Movies)
Φορτώσαμε δεδομένα από το **TMDB dataset**, το οποίο περιλαμβάνει τίτλους, βαθμολογίες και δημοτικότητα. 
- **Κλειδί (Key)**: Ο τίτλος της ταινίας (αφού περάσει από SHA-1).
- **Τιμή (Value)**: Ένα λεξικό (dictionary) με όλα τα στοιχεία της ταινίας.

### 8.2 Μηχανισμός Replication
Σε ένα πραγματικό δίκτυο, οι κόμβοι μπορεί να αποσυνδεθούν ξαφνικά. Αν μια ταινία ήταν αποθηκευμένη μόνο σε έναν κόμβο, θα χανόταν. Για να το αποφύγουμε αυτό, υλοποιήσαμε το **Replication**: κάθε ταινία αποθηκεύεται στον "υπεύθυνο" κόμβο αλλά και στους **$3$ επόμενους γείτονές του**.

#### Στον Chord:
Χρησιμοποιούμε τη **Successor List**. Κάθε κόμβος στέλνει αντίγραφα των δεδομένων του στους επόμενους κόμβους του δακτυλίου.

```python
# Από το chord_node.py
def replicate_data(self):
    # Στείλε τα δεδομένα σου σε κάθε κόμβο της λίστας διαδόχων
    for successor in self.successor_list:
        for key, value in self.data.items():
            successor.replicas[key] = value
```

#### Στον Pastry:
Χρησιμοποιούμε το **Leaf Set**. Τα αντίγραφα αποθηκεύονται στους αριθμητικά κοντινότερους κόμβους.

```python
# Από το pastry_node.py
def replicate_data(self):
    leaf_set = self.get_leaf_set()
    for node in leaf_set:
        # Αποθήκευση στο "replicas" storage του γείτονα
        for key, value in self.data.items():
            node.replicas[key] = value
```

### 8.3 Ανάκτηση Δεδομένων (Recovery)
Αν ένας κόμβος καταλάβει ότι ο προκάτοχός του (predecessor) έφυγε, μπορεί να ανακτήσει τα δεδομένα από τα δικά του αντίγραφα (`replicas`) και να γίνει αυτός ο νέος υπεύθυνος, διασφαλίζοντας ότι το δίκτυο παραμένει πλήρες.

Αυτή η προσέγγιση εγγυάται ότι ακόμα και αν "χάσουμε" μερικούς κόμβους ταυτόχρονα, οι ταινίες μας θα είναι ακόμα διαθέσιμες κάπου αλλού στο δίκτυο.

## 9. Πειράματα και Αποτελέσματα

Στο τελευταίο στάδιο, τρέξαμε μια σειρά από πειράματα για να δούμε πώς αποδίδουν τα δύο συστήματα στην πράξη. Χρησιμοποιήσαμε το movie dataset και μετρήσαμε τον αριθμό των **hops** (βημάτων) για χιλιάδες αναζητήσεις.

### 9.1 Σύγκριση Hops (Chord vs Pastry)
Το πιο σημαντικό γράφημα δείχνει πόσα βήματα χρειάζεται το κάθε πρωτόκολλο για να βρει μια ταινία όσο μεγαλώνει ο αριθμός των κλειδιών στο δίκτυο.

![Σύγκριση Hops](file:///d:/Ceid/7o%20Examino/Apokentromena/pastry_hops_comparison.png)

**Παρατηρήσεις:**
- Ο **Pastry** συνήθως χρειάζεται λιγότερα hops από τον Chord για το ίδιο μέγεθος δικτύου, λόγω της χρήσης του Routing Table με προθέματα (prefixes).
- Και τα δύο πρωτόκολλα διατηρούν μια λογαριθμική πολυπλοκότητα ($O(\log N)$), που σημαίνει ότι ακόμα και αν οι ταινίες γίνουν εκατομμύρια, ο χρόνος αναζήτησης δεν θα αυξηθεί δραματικά.

### 9.2 Κατανομή Δεδομένων
Ελέγξαμε αν η SHA-1 μοιράζει σωστά τις ταινίες στους κόμβους. Αν η κατανομή δεν ήταν ομοιόμορφη, κάποιοι κόμβοι θα "υπέφεραν" από υπερβολικό φορτίο.

![Κατανομή Κλειδιών](file:///d:/Ceid/7o%20Examino/Apokentromena/dht_hash_distribution_full.png)

Όπως φαίνεται στο γράφημα, οι ταινίες μοιράζονται σχεδώς ισόποσα σε όλους τους διαθέσιμους κόμβους, κάτι που επιβεβαιώνει ότι η επιλογή της SHA-1 ήταν σωστή.

### 9.3 Οπτικοποίηση Δικτύου (Chord Ring)
Για να κατανοήσουμε καλύτερα τη δομή, φτιάξαμε μια οπτικοποίηση του δακτυλίου του Chord, όπου φαίνονται οι κόμβοι και οι συνδέσεις (fingers) μεταξύ τους.

![Chord Network](file:///d:/Ceid/7o%20Examino/Apokentromena/chord_network_with_fingers.png)

Αυτά τα πειραματικά αποτελέσματα επιβεβαιώνουν ότι η υλοποίησή μας είναι σταθερή και αποδοτική, ακολουθώντας τις θεωρητικές προδιαγραφές των συστημάτων Chord και Pastry.

## 10. Συμπεράσματα

Η υλοποίηση της εργασίας ήταν μια πολύ καλή ευκαιρία να δούμε στην πράξη πώς λειτουργούν τα κατανεμημένα συστήματα μεγάλης κλίμακας. 

Μερικά από τα βασικά μας συμπεράσματα είναι:
- **P2P Δύναμη**: Καταφέραμε να διαχειριστούμε χιλιάδες ταινίες χωρίς κεντρικό server, μοιράζοντας το βάρος σε πολλούς κόμβους.
- **Chord vs Pastry**: Είδαμε ότι ενώ ο Chord είναι πιο εύκολος στην κατανόηση και στην υλοποίηση (δακτύλιος), ο Pastry είναι πιο αποδοτικός στη δρομολόγηση λόγω των προθεμάτων (Prefixes).
- **Αξιοπιστία**: Μέσω του Replication, το δίκτυο έγινε ανθεκτικό. Ακόμα και αν κάποιοι κόμβοι σταματήσουν να λειτουργούν, η πληροφορία παραμένει διαθέσιμη.
- **Τοπική Απόδοση**: Η χρήση του B+ Tree σε κάθε κόμβο έκανε τη διαφορά στις αναζητήσεις εντός του κόμβου, επιτρέποντάς μας να φιλτράρουμε τις ταινίες πολύ γρήγορα.

Συνολικά, το σύστημα που φτιάξαμε είναι πλήρως λειτουργικό και μπορεί να επεκταθεί ακόμα περισσότερο, π.χ. με πιο σύνθετους μηχανισμούς ασφάλειας ή δυναμική αυξομείωση του replication factor ανάλογα με τη δημοτικότητα της κάθε ταινίας.
